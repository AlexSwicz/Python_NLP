{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Disclaimer</b>  \n",
    "В целом работа не выполнена :(  \n",
    "Думал, что успею сделать за вечер, а в итоге долго провозился с установкой адаграма, да еще и безуспешно в общем-то. Поэтому прикладываю только какие-то зачатки работы, вдруг их хватит хотя бы на побалла. Больше в настолько долгий ящик я не буду откладывать (надеюсь). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n",
      "C:\\Users\\asus\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from lxml import html\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.decomposition import TruncatedSVD, NMF, PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import gensim\n",
    "import numpy as np\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from collections import Counter,defaultdict\n",
    "from string import punctuation\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "%matplotlib inline\n",
    "\n",
    "morph = MorphAnalyzer()\n",
    "punct = punctuation+'«»—…“”*№–'\n",
    "stops = set(stopwords.words('russian'))\n",
    "\n",
    "def normalize(text):\n",
    "    \n",
    "    words = [word.strip(punct) for word in text.lower().split()]\n",
    "    words = [morph.parse(word)[0].normal_form for word in words if word and word not in stops]\n",
    "\n",
    "    return ' '.join(words)\n",
    "\n",
    "def tokenize(text):\n",
    "    \n",
    "    words = [word.strip(punct) for word in text.lower().split()]\n",
    "\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'adagram'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-0b29563e958f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtfidf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0madagram\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m#сделал модель адаграмовскую через режим совместимости 10 винды с линуксом, даже сделал модель, а потом что-то пошло не так\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#в общем-то понятно что, в общем под виндой не импортируется, а времени уже нет доделывать\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named 'adagram'"
     ]
    }
   ],
   "source": [
    "tfidf=TfidfVectorizer()\n",
    "import adagram\n",
    "#сделал модель адаграмовскую через режим совместимости 10 винды с линуксом, даже сделал модель, а потом что-то пошло не так\n",
    "#в общем-то понятно что, в общем под виндой не импортируется, а времени уже нет доделывать"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rt = pd.read_csv('news_texts.csv', encoding = 'utf-8')\n",
    "data_rt.dropna(inplace=True)\n",
    "corpus = ' '.join(data_rt.content_norm)\n",
    "corpus_xml = html.fromstring(open('paraphraser/paraphrases.xml', 'rb').read())\n",
    "texts_1 = []\n",
    "texts_2 = []\n",
    "classes = []\n",
    "f = open('corpus.txt', 'w', encoding = 'utf-8')\n",
    "f.write(corpus)\n",
    "f.close()\n",
    "for p in corpus_xml.xpath('//paraphrase'):\n",
    "    texts_1.append(p.xpath('./value[@name=\"text_1\"]/text()')[0])\n",
    "    texts_2.append(p.xpath('./value[@name=\"text_2\"]/text()')[0])\n",
    "    classes.append(p.xpath('./value[@name=\"class\"]/text()')[0])\n",
    "    \n",
    "data = pd.DataFrame({'text_1':texts_1, 'text_2':texts_2, 'label':classes})\n",
    "data['text_1_norm'] = data['text_1'].apply(normalize)\n",
    "data['text_2_norm'] = data['text_2'].apply(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>SVD</b>  \n",
    "Берем все из тетрадки 4 семинара."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(min_df=3, max_df=0.4, max_features=1000)\n",
    "X = cv.fit_transform(data_rt['content_norm'])\n",
    "id2word = {i:w for i,w in enumerate(tfidf.get_feature_names())}\n",
    "word2id = {w:i for i,w in id2word.items()}\n",
    "svd = TruncatedSVD(200)\n",
    "svd.fit(X)\n",
    "id2vec_svd = svd.components_.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>NMF</b>  \n",
    "Берем оттуда же."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf = NMF(50)\n",
    "nmf.fit(X)\n",
    "id2vec_nmf = nmf.components_.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Word2Vec</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = gensim.models.Word2Vec([text.split() for text in data_rt['content_norm']], size=50, sg=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Fasttext</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_text = gensim.models.FastText([text.split() for text in data_rt['content_norm']], size=50, min_n=4, max_n=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Adagram</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'adagram' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-d00ecde10f9b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mvm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madagram\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVectorModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"out.pkl\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'adagram' is not defined"
     ]
    }
   ],
   "source": [
    "vm = adagram.VectorModel.load(\"out.pkl\")\n",
    "#притворимся, что этого не было"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Косинусные близости</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar(word, id2vec):\n",
    "    similar = [id2word[i] for i in cosine_distances(id2vec[word2id[word]].reshape(1, -1), id2vec).argsort()[0][:10]]\n",
    "    return similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['победа',\n",
       " 'одержать',\n",
       " 'победить',\n",
       " 'успех',\n",
       " 'выиграть',\n",
       " 'добиться',\n",
       " 'поражение',\n",
       " 'настоящий',\n",
       " 'уверенный',\n",
       " 'суметь']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar('победа', id2vec_svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['победа',\n",
       " 'второе',\n",
       " 'одержать',\n",
       " 'первое',\n",
       " 'суметь',\n",
       " 'три',\n",
       " 'поражение',\n",
       " 'четыре',\n",
       " 'проиграть',\n",
       " 'удаться']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar('победа', id2vec_nmf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('одержать', 0.9100478291511536),\n",
       " ('победить', 0.8394150137901306),\n",
       " ('одерживать', 0.8190760016441345),\n",
       " ('выиграть', 0.81434166431427),\n",
       " ('успех', 0.8041506409645081),\n",
       " ('проигрыш', 0.7821857929229736),\n",
       " ('волевой', 0.7723301649093628),\n",
       " ('отпраздновать', 0.7673037052154541),\n",
       " ('одолеть', 0.765396237373352),\n",
       " ('поражение', 0.7642831802368164)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar('победа')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('победно', 0.9180482029914856),\n",
       " ('побег', 0.9014240503311157),\n",
       " ('беда', 0.86541348695755),\n",
       " ('победоносец', 0.863079845905304),\n",
       " ('обед', 0.7952977418899536),\n",
       " ('плевок', 0.7758498191833496),\n",
       " ('побои', 0.7573943138122559),\n",
       " ('победитель', 0.7384722828865051),\n",
       " ('выигрыш', 0.7329931855201721),\n",
       " ('поражение', 0.7200791835784912)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fast_text.most_similar('победа')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Обучающая выборка и модель</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_text_1 = svd.fit_transform(tfidf.transform(data['text_1_norm']))\n",
    "X_text_2 = svd.fit_transform(tfidf.transform(data['text_2_norm']))\n",
    "X_text = np.concatenate([X_text_1, X_text_2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.56      0.56      0.56       629\n",
      "          0       0.49      0.49      0.49       737\n",
      "          1       0.36      0.35      0.36       441\n",
      "\n",
      "avg / total       0.48      0.48      0.48      1807\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_X, valid_X, train_y, valid_y = train_test_split(X_text, y, random_state=1)\n",
    "clf = RandomForestClassifier(n_estimators=100, max_depth=10, min_samples_leaf=10,\n",
    "                             class_weight='balanced')\n",
    "clf.fit(train_X, train_y)\n",
    "preds = clf.predict(valid_X)\n",
    "print(classification_report(valid_y, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_text_1 = nmf.fit_transform(tfidf.transform(data['text_1_norm']))\n",
    "X_text_2 = nmf.fit_transform(tfidf.transform(data['text_2_norm']))\n",
    "X_text = np.concatenate([X_text_1, X_text_2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.49      0.56      0.52       629\n",
      "          0       0.46      0.37      0.41       737\n",
      "          1       0.32      0.36      0.34       441\n",
      "\n",
      "avg / total       0.44      0.43      0.43      1807\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_X, valid_X, train_y, valid_y = train_test_split(X_text, y,random_state=1)\n",
    "clf = RandomForestClassifier(n_estimators=100, max_depth=7, min_samples_leaf=15,\n",
    "                             class_weight='balanced')\n",
    "clf.fit(train_X, train_y)\n",
    "preds = clf.predict(valid_X)\n",
    "print(classification_report(valid_y, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text, model, dim):\n",
    "    text = text.split()\n",
    "\n",
    "    words = Counter(text)\n",
    "    total = len(text)\n",
    "    vectors = np.zeros((len(words), dim))\n",
    "    \n",
    "    for i,word in enumerate(words):\n",
    "        try:\n",
    "            v = model[word]\n",
    "            vectors[i] = v*(words[word]/total)\n",
    "        except (KeyError, ValueError):\n",
    "            continue\n",
    "    \n",
    "    if vectors.any():\n",
    "        vector = np.average(vectors, axis=0)\n",
    "    else:\n",
    "        vector = np.zeros((dim))\n",
    "    \n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "dim = 50\n",
    "X_text_1_w2v = np.zeros((len(data['text_1_norm']), dim))\n",
    "X_text_2_w2v = np.zeros((len(data['text_2_norm']), dim))\n",
    "\n",
    "for i, text in enumerate(data['text_1_norm'].values):\n",
    "    X_text_1_w2v[i] = get_embedding(text, w2v, dim)\n",
    "    \n",
    "for i, text in enumerate(data['text_2_norm'].values):\n",
    "    X_text_2_w2v[i] = get_embedding(text, w2v, dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_text_w2v = np.concatenate([X_text_1_w2v, X_text_2_w2v], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.56      0.51      0.54       629\n",
      "          0       0.46      0.46      0.46       737\n",
      "          1       0.38      0.43      0.40       441\n",
      "\n",
      "avg / total       0.48      0.47      0.47      1807\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_X, valid_X, train_y, valid_y = train_test_split(X_text_w2v, y,random_state=1)\n",
    "clf = RandomForestClassifier(n_estimators=100, max_depth=7, min_samples_leaf=15,\n",
    "                             class_weight='balanced')\n",
    "clf.fit(train_X, train_y)\n",
    "preds = clf.predict(valid_X)\n",
    "print(classification_report(valid_y, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "dim = 50\n",
    "data['text_1_notnorm'] = data['text_1'].apply(tokenize)\n",
    "data['text_2_notnorm'] = data['text_2'].apply(tokenize)\n",
    "\n",
    "X_text_1_ft = np.zeros((len(data['text_1_notnorm']), dim))\n",
    "X_text_2_ft = np.zeros((len(data['text_2_notnorm']), dim))\n",
    "\n",
    "for i, text in enumerate(data['text_1_notnorm'].values):\n",
    "    X_text_1_ft[i] = get_embedding(text, fast_text, dim)\n",
    "    \n",
    "for i, text in enumerate(data['text_2_notnorm'].values):\n",
    "    X_text_2_ft[i] = get_embedding(text, fast_text, dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_text_ft = np.concatenate([X_text_1_ft, X_text_2_ft], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.54      0.50      0.52       629\n",
      "          0       0.48      0.54      0.50       737\n",
      "          1       0.40      0.35      0.38       441\n",
      "\n",
      "avg / total       0.48      0.48      0.48      1807\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_X, valid_X, train_y, valid_y = train_test_split(X_text_ft, y,random_state=1)\n",
    "clf = RandomForestClassifier(n_estimators=100, max_depth=7, min_samples_leaf=15,\n",
    "                             class_weight='balanced')\n",
    "clf.fit(train_X, train_y)\n",
    "preds = clf.predict(valid_X)\n",
    "print(classification_report(valid_y, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
